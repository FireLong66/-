

# 数值优化概念

## 问题本质

就是从一个初始值出发，根据目标函数的局部信息作为指导根据更新规则来找到下一个解，不断迭代，一直找到使得目标函数最小（大）的自变量。其实就是给一个关于自变量x的目标函数，可能带约束，反找出满足约束且使得目标函数最优（次优）的自变量x的解。



## **核心挑战**

问题本质是很简单的，难点在于算法收敛性。

高效利用局部信息

收敛性与效率的冲突

如何处理约束与目标的冲突

全局最优性保证



## **优化过程**

数值优化的核心任务，本质上是对 “目标函数与自变量的映射关系” 进行 “逆向求解”：
已知：目标函数f(x)（输入是自变量 x，输出是衡量优劣的指标），以及可能的约束条件限制x 的取值范围）。
求解：找到一个x∗，使得在满足所有约束的前提下，f(x∗) 达到最小（或最大）—— 即 “反推” 出能让目标函数最优的自变量取值。



将优化算法视为一个**动态系统**：

- **输入**：目标函数f(x)、约束条件、初始点x_0。
- **内部机制**：
  通过局部信息生成搜索方向（如梯度方向、牛顿方向）和步长（如学习率、信赖域半径）。
- **输出**：收敛点 x^*局部最优或全局最优）。



## **优化算法的“不可能三角”**

**收敛速度**、**计算成本**、**适用范围**



## 数值优化分类

### 按约束条件分类

#### 无约束优化（Unconstrained Optimization）

- **定义**：目标函数仅依赖于变量，无任何限制条件

#### 约束优化（Constrained Optimization）

- **定义**：变量需满足一定的约束条件

### 按目标函数数量分类

#### 1. 单目标优化（Single-Objective Optimization）

- **定义**：仅需优化一个目标函数，目标明确（如最小化成本、最大化效率）。
- **特点**：最优解通常唯一（或在一个连续区域内），可通过传统方法直接求解。

#### 2. 多目标优化（Multi-Objective Optimization）

- **定义**：需同时优化多个目标函数（可能相互冲突，如成本与性能）。

- **特点**：不存在绝对最优解，而是**帕累托最优解**（无法在改进一个目标的同时不损害其他目标）。

  

### 按目标函数和约束的性质分类

根据函数的线性、凸性等性质，可分为：

#### 1. 线性优化（Linear Optimization/Linear Programming, LP）

- **定义**：目标函数和所有约束均为线性函数
- **典型方法**：单纯形法、内点法。
- **应用场景**：资源分配、生产计划等线性模型问题。

#### 2. 非线性优化（Nonlinear Optimization）

- **定义**：目标函数或约束中至少有一个是非线性函数。
- **典型方法**：牛顿法、信赖域方法、序列二次规划（SQP）。
- **应用场景**：非线性模型拟合、工程中的非线性设计优化（如曲线拟合、结构力学分析）。

#### 3. 凸优化（Convex Optimization）

- **定义**：目标函数为凸函数，可行域（约束条件构成的集合）为凸集。
- **特点**：局部最优解即为全局最优解，求解难度低，收敛性有保障。
- **典型问题**：线性规划、二次规划（正定矩阵）、凸二次约束优化等。
- **应用场景**：机器学习中的支持向量机（SVM）、最小二乘问题、信号处理等。

####  4.非凸优化（Nonconvex Optimization）

- **定义**：目标函数非凸或可行域非凸，存在多个局部最优解，全局最优解难以求解。
- 典型方法：
  - 启发式算法（遗传算法、模拟退火、粒子群优化）
  - 分支定界法（通过分割可行域寻找全局最优）
- **应用场景**：组合优化问题（如旅行商问题）、神经网络训练（非凸损失函数）等。



### 按变量类型分类

#### 1. 连续优化（Continuous Optimization）

- **定义**：变量取值为连续实数（*x*∈R*n*），是最常见的优化类型。
- **示例**：无约束优化、线性规划、非线性规划等。

#### 2. 离散优化（Discrete Optimization）

- **定义**：变量取值为离散值（如整数、二进制 0/1）。
- 子类：
  - 整数规划（变量为整数）
  - 0-1 规划（变量仅取 0 或 1，如选址问题、决策问题）
  - 组合优化（变量为离散集合中的元素，如排列、子集，如旅行商问题 TSP）
- **典型方法**：分支定界法、动态规划、启发式算法（遗传算法、蚁群算法）。
- **应用场景**：调度问题（如任务分配）、网络设计（如节点连接选择）等。

#### 3. 混合整数优化（Mixed-Integer Optimization）

- **定义**：部分变量为连续值，部分为离散值（如整数或 0/1），如混合整数线性规划（MILP）。
- **应用场景**：含离散决策的资源分配（如是否建设工厂的 0/1 变量 + 生产数量的连续变量）。







# 无约束优化

基于目标函数的一阶导数（梯度）寻找最优解，通过沿梯度负方向迭代更新变量，逐步逼近最小值。

## 线搜索法（一维搜索）

### 原理

​	在优化过程的每一步，先确定搜索方向（由梯度、牛顿方向等方法给出），再沿该方向寻找使目标函数值下降最多的步长。 

线搜索的本质是将多维优化转化为一维问题：固定方向 dk，寻找最优步长 *α*≥0。

### **精确线搜索（Exact Line Search）**

**准则**：在搜索方向上找到使目标函数值最小的步长，即：

*α*∗=arg*α*>0 min*f*(xk+αdk)



### 非精确线搜索（Inexact Line Search）

通过满足一定条件来近似选择步长，平衡计算效率和收敛性。

必要但不充分：f(xk+1)>f(xk),可能选择的xk序列，能保证f(xk)下降收敛，但不一定收敛到极小值。xk序列的选择也有影响。

#### 核心准则

为保证目标函数单调下降且迭代收敛，步长 *α* 需满足以下条件

1. Armijo 准则（充分下降条件）确保 “足够下降”，拒绝无效步长
   核心思想：确保步长能使目标函数 “足够下降”，避免步长过大导致函数值下降不足。限制步长的上界。

  直观理解：想象沿某方向下山，Armijo 准则要求 “每一步至少要向下走够一定距离”（比如线性近似能走 10 米，实际至少走 1 米），防止迈太大步反而往上滑，或方向错误根本没下山。

2. Goldstein 准则：双向限制步长，避免过短或过长

​	核心思想：在 Armijo 准则的基础上增加 “下限约束”，既防止步长过大（函数值下降不足），也防止步长过小（函数值下降过多，反而可能错过更优区域），将步长限制在一个合理的区间内。

​	直观理解：仍以下山为例，Goldstein 准则不仅要求 “至少走 1 米”（Armijo 的上限），还要求 “最多走 9 米”（下限，假设线性近似能走 10 米），避免步子太小（0.1 米）或太大（11 米），确保步长在 “1-9 米” 的合理范围。

3. Wolfe 准则：用曲率条件限制步长，兼顾下降速度

​	核心思想：不直接限制函数值的下降幅度，而是通过 “梯度变化”（曲率条件）判断步长是否合理 —— 既保证函数值足够下降（Armijo 准则），又避免步长停留在梯度下降过快的区域（防止步长过小）。

​	直观理解：下山时，除了 “至少走 1 米”（Armijo），还要求 “每一步后的陡峭程度（梯度）不能比原来陡太多”（比如原来的陡峭度是 - 10（负号表示下降），新的陡峭度至少是 - 9，即下降速度不能比原来快太多）。这避免了步长过小导致 “一直在陡坡上小步挪”，提高迭代效率。

​	**曲率条件要求新点的斜率（梯度沿搜索方向的分量）“不能比原来的斜率更陡太多”，即斜率的绝对值不能增大得太夸张，最好是往平缓的方向变化（绝对值减小），以此避免步长过小导致迭代卡在 “陡坡区域”**。避免步长过小拖慢速度

​	**别在陡坡上迈小碎步，要么迈大一点到平缓区，要么即使还在稍陡的地方，也得保证没陡得太离谱**。

​	优势：比 Goldstein 准则更灵活，能保留更多潜在优质步长，广泛应用于共轭梯度法、拟牛顿法等高效算法中。



| 准则      | 核心约束对象          | 解决的问题             | 典型应用场景             |
| --------- | --------------------- | ---------------------- | ------------------------ |
| Armijo    | 函数值下降的下限      | 避免步长过大或方向错误 | 简单梯度下降法           |
| Goldstein | 函数值下降的上下限    | 避免步长过大或过小     | 对步长范围要求严格的场景 |
| Wolfe     | 函数值下降 + 梯度变化 | 平衡下降幅度与步长效率 | 共轭梯度法、拟牛顿法等   |

---

## Lipschitz 连续（利普希茨条件）

### 定义

使得对任意 \(x_1, x_2)，满足∥*f*(*x*1)−*f*(*x*2)∥≤*L*⋅∥*x*1−*x*2∥

则称 f 在 D 上是 **Lipschitz 连续** 的，常数 L 称为 f 的 **Lipschitz 常数**（最小的 L 称为最佳 Lipschitz 常数）

### 几何意义

**函数图像上任意两点连线的斜率绝对值不超过 L**。即函数的变化不会 “太陡峭”，确保函数行为可控,被斜率为+- L的两条直线 “夹在中间”。

 **“函数变化不会太剧烈”**，**开凸定义域**和 **函数可微** 的前提下， 多元函数，Lipschitz 连续等价于函数梯度有界。

### 性质

1. **稳定性**：若 \(f, g\) 分别以 \(L_1, L_2\) 为 Lipschitz 常数，则：
   - \(af + bg\)（\(a,b\) 为常数）的 Lipschitz 常数为 \(|a|L_1 + |b|L_2\)；
   - 复合函数 \(f(g(x))\) 的 Lipschitz 常数为 \(L_1* L_2\)（若 g 的值域在 f 的定义域内）。
2. **收缩映射**：当 \(0 \leq L < 1\) 时，称为 **Lipschitz 收缩映射**。这类函数满足 \(||f(x_1) - f(x_2) || < \|| x_1 - x_2 ||\)，即映射后两点距离缩小，是不动点定理（如 Banach 不动点定理）的核心条件。

### 应用场景

- **微分方程解的存在性**：若微分方程 \(y' = f(x,y)\) 中的 f 关于 y 满足 Lipschitz 条件，则解存在且唯一（皮卡 - 林德洛夫定理）。

- **优化算法收敛性**：在梯度下降中，若目标函数是 Lipschitz 连续的，则可证明算法的收敛速率（如线性收敛）。

- **机器学习**：损失函数的 Lipschitz 连续性可保证训练过程的稳定性，避免梯度爆炸。

  

### 与其他连续性的关系

**Lipschitz 连续 ⇒ 一致连续 ⇒ 连续**：

**可导函数的 Lipschitz 条件**：
若 f 在区间 \([a,b]\) 上可导，且导数有界（\(|f'(x)|<= L\)），则 f 是 Lipschitz 连续的（由拉格朗日中值定理可证）。

反之，Lipschitz 连续的函数不一定可导（例如 \(f(x) = |x|\) 在 \(x=0\) 不可导，但 \(L=1\) 时满足 Lipschitz 条件）。

结论：**对于可导函数来说，导数有界等价于 Lipschitz 连续**

---

## 梯度下降法

基于目标函数的一阶导数（梯度）寻找最优解，通过沿梯度负方向迭代更新变量，逐步逼近最小值。

## 牛顿法及改进（二阶方法）

利用目标函数的二阶导数（Hessian 矩阵）提供曲率信息，收敛速度快于一阶方法，但计算复杂度高。
